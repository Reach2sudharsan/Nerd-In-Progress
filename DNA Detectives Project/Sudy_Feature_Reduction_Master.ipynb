{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sudy_Feature_Reduction_Master",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtjHts0etN7m"
      },
      "source": [
        "# Goals\n",
        "In this colab you will:\n",
        "\n",
        "*   Learn what feature reduction is and why it is important.\n",
        "*   Learn about Lasso (L1) normalization, one of the most common methods of feature reduction.\n",
        "*   Implement Lasso feature reduction into your own model and evaluate it.\n",
        "*   Identify important mutations that can be used as biomarkers in determining where SARS-CoV-2 samples came from.\n",
        "*  Learn the basics of Principal Components Analysis, a form of unsupervised learning, and apply it to your dataset.\n",
        "\n",
        "***Note: This notebook covers some advanced topics typically taught in college-level Machine Learning courses. Don't worry if you don't understand the nitty gritty parts of the math. It is more important you understand general concepts, such as why feature reduction is important. Don't hesitate to ask your instructor any questions that come up!***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGz8a54mb5qc",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "8133b687-7966-4a9a-a56a-554fc5e964bd"
      },
      "source": [
        "#@title ## Set up the notebook\n",
        "!pip install Biopython\n",
        "from Bio import SeqIO\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn import model_selection, linear_model\n",
        "from sklearn import decomposition\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import gdown\n",
        "data_path = 'https://drive.google.com/uc?id=1f1CtRwSohB7uaAypn8iA4oqdXlD_xXL1'\n",
        "cov2_sequences = 'SARS_CoV_2_sequences_global.fasta'\n",
        "gdown.download(data_path, cov2_sequences, True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Biopython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/02/8b606c4aa92ff61b5eda71d23b499ab1de57d5e818be33f77b01a6f435a8/biopython-1.78-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Biopython) (1.19.4)\n",
            "Installing collected packages: Biopython\n",
            "Successfully installed Biopython-1.78\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'SARS_CoV_2_sequences_global.fasta'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NxWdGoIQZUA",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67600077-31ee-4a8a-b53b-efc95efd1ba3"
      },
      "source": [
        "#@title Load data from previous notebook\n",
        "sequences = [r for r in SeqIO.parse(cov2_sequences, 'fasta')]\n",
        "# This can take a couple minutes!\n",
        "#@title ####Example Solution\n",
        "# Note: This can take a couple minutes to run! \n",
        "# but we can monitor our progress using the tqdm library\n",
        "mutation_df = pd.DataFrame()\n",
        "n_bases_in_seq = len(sequences[0])\n",
        "\n",
        "print(\"Creating feature matrix....\")\n",
        "# Iterate though all positions in this sequence.\n",
        "for location in (range(n_bases_in_seq)): # tqdm is a nice library that prints our progress.\n",
        "  bases_at_location = np.array([s[location] for s in sequences])\n",
        "  # If there are no mutations at this position, move on.\n",
        "  if len(set(bases_at_location))==1: continue # If\n",
        "  for base in ['A', 'T', 'G', 'C', '-']:\n",
        "    feature_values = (bases_at_location==base)\n",
        "    \n",
        "    \n",
        "    # Set the values of any base that equals 'N' to np.nan.\n",
        "    feature_values[bases_at_location=='N'\n",
        "                   ] = np.nan\n",
        "    \n",
        "    # Convert from T/F to 0/1.\n",
        "    feature_values  = feature_values*1\n",
        "    \n",
        "    # Make the column name look like <location>_<base> (1_A, 2_G, 3_A, etc.)\n",
        "    column_name = str(location) + '_' + base\n",
        "    mutation_df[column_name] = feature_values\n",
        "\n",
        "print(\"Formatting labels....\")\n",
        "countries = [(s.description).split('|')[-1] for s in sequences]\n",
        "countries_to_regions_dict = {\n",
        "         'Australia': 'Oceania',\n",
        "         'China': 'Asia',\n",
        "         'Hong Kong': 'Asia',\n",
        "         'India': 'Asia',\n",
        "         'Nepal': 'Asia',\n",
        "         'South Korea': 'Asia',\n",
        "         'Sri Lanka': 'Asia',\n",
        "         'Taiwan': 'Asia',\n",
        "         'Thailand': 'Asia',\n",
        "         'USA': 'North America',\n",
        "         'Viet Nam': 'Asia'\n",
        "}\n",
        "regions = [countries_to_regions_dict[c] if c in \n",
        "           countries_to_regions_dict else 'NA' for c in countries]\n",
        "mutation_df['label'] = regions\n",
        "\n",
        "print(\"Balacing data labels....\")\n",
        "balanced_df = mutation_df.copy()\n",
        "balanced_df['label'] = regions\n",
        "balanced_df = balanced_df[balanced_df.label!='NA']\n",
        "balanced_df = balanced_df.drop_duplicates()\n",
        "samples_north_america = balanced_df[balanced_df.label== ####### FILL IN ####\n",
        "                                    'North America']\n",
        "samples_oceania = balanced_df[balanced_df.label== ##### FILL IN #########\n",
        "                              'Oceania']\n",
        "samples_asia = balanced_df[balanced_df.label== ##### FILL IN #######\n",
        "                           'Asia']\n",
        "\n",
        "# Number of samples we will use from each region.\n",
        "n = min(len(samples_north_america),\n",
        "        len(samples_oceania),\n",
        "        len(samples_asia))\n",
        "\n",
        "balanced_df = pd.concat([samples_north_america[:n],\n",
        "                    samples_asia[:n],\n",
        "                    samples_oceania[:n]])\n",
        "\n",
        "X = balanced_df.drop('label', 1)\n",
        "Y = balanced_df.label\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating feature matrix....\n",
            "Formatting labels....\n",
            "Balacing data labels....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSbUyrhixFqr",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4896e8-27a6-48c9-8d31-5d34d7f82edf"
      },
      "source": [
        "#@title #### Retrain previous model\n",
        "lm = linear_model.LogisticRegression(\n",
        "    multi_class=\"multinomial\", max_iter=1000,\n",
        "    fit_intercept=False, tol=0.001, solver='saga', random_state=42)\n",
        "\n",
        "# Split into training/testing set.\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(\n",
        "    X, Y, train_size=.8, random_state=42)\n",
        "\n",
        "# Train/fit model.\n",
        "lm.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=False,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=42, solver='saga', tol=0.001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOU4_ARYQhcn"
      },
      "source": [
        "# Feature-reduced Models\n",
        "\n",
        "Even in regular logistic regression, features may or may not get used in a model. Recall the equation logistic regression is fitting:\n",
        "\n",
        "$\n",
        "Y = H(\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...)\n",
        "$\n",
        "\n",
        "After training, if any $\\beta$'s equal 0, that means the corresponding $x$ feature *did not get used in the model*.  ***The number of features used in the model equals the number of non-zero coefficients ($\\beta$).***\n",
        "\n",
        "We can access the values of the coefficients after training with ```lm.coef_[0]```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63vx9XE2Rixz"
      },
      "source": [
        "### **Exercise: Recall your previous model. How many *features* did we use in the model?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGY3pqIEzXwM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c124d14c-5ef5-4772-dff3-d042a01059f3"
      },
      "source": [
        "coefficients = lm.coef_[0]#### FILL IN ######\n",
        "n_possible_features = len(coefficients)\n",
        "\n",
        "n_features_used = 0\n",
        "\n",
        "\n",
        "for i in coefficients:\n",
        "  if i != 0:\n",
        "    n_features_used+=1\n",
        "\n",
        "\n",
        "\n",
        "print(\"The original logistic regression model used %i out of a possible %i features\" % \n",
        "      (n_features_used, n_possible_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The original logistic regression model used 4377 out of a possible 12680 features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0kDZ4D3z0jh"
      },
      "source": [
        "Thousands of features is a lot of features! In feature-reduced models, we penalize the model for having more features. Hopefully then the model will pick only a few very important features to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmHdTsxxuA3g",
        "cellView": "form"
      },
      "source": [
        "#@title ### **Exercise: Why might we want to try to *reduce* the number of features we use in a model?**\n",
        "_1_ = \"Decrease processing time\" #@param {type:\"string\"}\n",
        "_2_ = \"Decrease memory that we use, no overfitting\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"1. To prevent overfitting. Especially in models with more features than samples, \")\n",
        "print(\"we can end up overfitting on the training set.\\n\")\n",
        "print(\"2. In order to determine which features are the most important.\")\n",
        "print(\"In the case of SARS-CoV-2, we can use the important features as \")\n",
        "print(\"biomarkers to determine which lineage came from which region.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YFlWINj4fJD"
      },
      "source": [
        "### **Excercise: Compute training and testing accuracy to see if your original model was overfitting.**. \n",
        "\n",
        "What does it mean if you're training accuracy is significantly higher than your testing accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_7GSGAj6BSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8fca23b-7179-4cb3-bedf-c54029689ebf"
      },
      "source": [
        "# Note that we have already loaded lm, X_test, X_train, Y_train, and Y_test for you!\n",
        "Y_pred_train = lm.predict(X_train)\n",
        "Y_pred_test = lm.predict(X_test)#### FILL IN ######\n",
        "\n",
        "\n",
        "n = 0 \n",
        "for i in range(Y_pred_train.shape[0]):\n",
        "  if Y_pred_train[i] == Y_train.iloc[i]:\n",
        "    n+=1\n",
        "\n",
        "training_accuracy  = n/(Y_pred_train.shape[0])###### FILL IN #########\n",
        "\n",
        "\n",
        "n = 0 \n",
        "for i in range(Y_pred_test.shape[0]):\n",
        "  if Y_pred_test[i] == Y_test.iloc[i]:\n",
        "    n+=1\n",
        "\n",
        "testing_accuracy = n/(Y_pred_test.shape[0])###### FILL IN #########\n",
        "\n",
        "\n",
        "print(\"Training accuracy: %\", training_accuracy)\n",
        "print(\"Testing accuracy: %\", testing_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: % 0.993485342019544\n",
            "Testing accuracy: % 0.9090909090909091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuONTGFVSy_t"
      },
      "source": [
        "# The Lasso\n",
        "\n",
        "![alt text](https://pngimg.com/uploads/wonder_woman/wonder_woman_PNG30.png)\n",
        "\n",
        "\n",
        "One way of reducing the feature space is to use a **Lasso** (or L1) penalty.\n",
        "\n",
        "If a logistic regression model takes the form of\n",
        "\n",
        "$\n",
        "Y = H(\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...)\n",
        "$\n",
        "\n",
        "Then a lasso penalty enforces a penalty that aims to make as many $\\beta$ terms equal zero. After this penalty, the $x$ features that correspond to the non-zero $\\beta$ terms left will be the most 'important' features in your model.\n",
        "\n",
        "If you're interested in the math, we implement this by using a cost/loss function:\n",
        "\n",
        "$\n",
        "Loss = Cost(X, Y, \\beta) + \\lambda \\sum_{i}|\\beta_i|\n",
        "$\n",
        "\n",
        "Different values of $\\lambda$ will make the model force more or less of the coefficients ($\\beta$ values) to 0.\n",
        "\n",
        "You can learn more [here](https://towardsdatascience.com/over-fitting-and-regularization-64d16100f45c) and [here](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1MdTbR3Wp8v"
      },
      "source": [
        "## Build a Lasso-Penalized Model\n",
        "The ```linear_model.LogisticRegression``` class has an easy way to add in a Lasso penalty.\n",
        "\n",
        "Add an argument ```penalty='l1'``` to tell the model to use a lasso penalty when training. Add an argument ```C=1/lam``` and pick a different value of ```lam``` to enforce a bigger or smaller penalty on the model for using coefficients.\n",
        "\n",
        "```\n",
        "l1m = linear_model.LogisticRegression(\n",
        "    multi_class=\"multinomial\", max_iter=1000,\n",
        "    fit_intercept=False, tol=0.001, C=c,\n",
        "    penalty='l1', solver='saga', random_state=42)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIpgjWh07jzU"
      },
      "source": [
        "### **Exercise: Investigate how different values of $\\lambda$ affect the number of features in a model and its accuracy.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJPciLyhXMPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cbaa30-543a-4595-fb7a-116b679c8188"
      },
      "source": [
        "lam = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "l1m = linear_model.LogisticRegression(\n",
        "    multi_class=\"multinomial\", max_iter=1000,\n",
        "    fit_intercept=False, tol=0.001, C=1/lam,\n",
        "    penalty='l1', solver='saga', random_state=42)\n",
        "l1m.fit(X_train, Y_train)\n",
        "print(\"Using lambda=\", lam)\n",
        "print(\"Lasso Training accuracy:\", np.mean(Y_train==l1m.predict(X_train)))\n",
        "print(\"Lasso Testing accuracy:\", np.mean(Y_test==l1m.predict(X_test)))\n",
        "print(\"Number of non-zero coefficients in lasso model:\", sum(l1m.coef_[0]!=0))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using lambda= 0.7\n",
            "Lasso Training accuracy: 0.9771986970684039\n",
            "Lasso Testing accuracy: 0.922077922077922\n",
            "Number of non-zero coefficients in lasso model: 132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-svMNftaIxl"
      },
      "source": [
        "# Cross Validation \n",
        "\n",
        "We can a method called cross validation to decide what the best value of $\\lambda$ is. In cross validation, we split the training set into a training and validation set. We train the model using different values of $\\lambda$ (or $C$) and evaluate the accuracy using the validation set. We do this using $K$ different iterations of testing/validation split. Then we choose the value of $\\lambda$ that has the best average accuracy for the final output of the training.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/1280px-K-fold_cross_validation_EN.svg.png\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "Instead of specifying ```C=1/lam``` in our arguments to the ```LogisticRegression``` class, we can use ```LogisticRegressionCV``` class the argument ```cv=``` and set the number of C (or 1/$\\lambda$) values we wish for the model to go through.\n",
        "\n",
        "```\n",
        "lm_cv = linear_model.LogisticRegressionCV(\n",
        "    multi_class=\"multinomial\", max_iter=1000,\n",
        "    fit_intercept=False, tol=0.001,\n",
        "    solver='saga', random_state=42,\n",
        "    Cs=3, penalty='l1')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH71Q9ax8uao"
      },
      "source": [
        "### **Exercise: Implement cross-validation to the logistic regression model and compute accuracy.**\n",
        "\n",
        "Note: Sometimes you can get unlucky with the randomly selected validation sets, and cross validation won't improve your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPBQtSOD92wG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ef910f-754e-4fed-e469-7bb3960b3a9d"
      },
      "source": [
        "lm_cv = linear_model.LogisticRegressionCV(\n",
        "    multi_class=\"multinomial\", max_iter=1000,\n",
        "    fit_intercept=False, tol=0.001,\n",
        "    solver='saga', random_state=42,\n",
        "    Cs=5, penalty ='l1'\n",
        "    ### FILL IN ###  #Specify the L1 penalty and set the number of Cs iterations to 5. \n",
        "    )\n",
        "\n",
        "lm_cv.fit(X_train, Y_train)\n",
        "print(\"Training accuracy:\", np.mean(Y_train==lm_cv.predict(X_train)))\n",
        "print(\"Testing accuracy:\", np.mean(Y_test==lm_cv.predict(X_test)))\n",
        "print(\"Number of non-zero coefficients in lasso model:\", sum(lm_cv.coef_[0]!=0))\n",
        "print(\"Lambda decided on by cross validation:\", 1/lm_cv.C_[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.9609120521172638\n",
            "Testing accuracy: 0.922077922077922\n",
            "Number of non-zero coefficients in lasso model: 101\n",
            "Lambda decided on by cross validation: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tnwUWuRQvZF"
      },
      "source": [
        "# Bonus Section: Unsupervised Learning\n",
        "\n",
        "Although we have focused on supervised machine learning in this class, biology and genomics often uses a lot of unsupervised learning.\n",
        "\n",
        "Unsupervised learning is another way to reduce the number of features in a dataset. Instead of throwing out features, we create new features that are linear combinations of the original features.\n",
        "\n",
        "<img src=\"https://blog.paralleldots.com/wp-content/uploads/2018/03/fig_pca_illu3d-1024x406.png\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        "For example, say we have a dataset with features ```arm length```, ```leg length```, ```sprint speed```, and ```mile speed```.\n",
        "\n",
        "|Name |Arm Length |Leg Length | Sprint Speed| Km Speed|\n",
        "|--|--|--|--|--|\n",
        "|Jocelyn|89cm|104cm|30kph|20kph|\n",
        "|Aisha|99cm |120cm|29kph|19kph|\n",
        "|Su Lin| 90cm|106cm|26kph|18kph|\n",
        "|Marissa|100cm|122cm|29kph|20kph|\n",
        "\n",
        "Principal component analysis (PCA) is a method that allows us to find linear combinations of features that spreads the data furthest apart on a lower dimensional grid.\n",
        "\n",
        "If we used PCA to reduce the feature space into 2 features (instead of 4), PCA might tell us that the best new features to use would be:\n",
        "\n",
        "|Name |Arm + Leg Length |Sprint Speed + Km Speed|\n",
        "|--|--|--|\n",
        "|Jocelyn|193cm|50kph\n",
        "|Aisha|219cm|48kph|\n",
        "|Su Lin|196cm|44kph|\n",
        "|Marissa|222cm|39kph|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzChRtUkEMsD"
      },
      "source": [
        "Read [this article](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c) and play around with the widget to learn more about PCA!\n",
        "\n",
        "\n",
        "In biology, we normally use unsupervised learning as a way to visualize data that is high-dimensional (large # of features) in a 2-dimensional grid. We can then look for data points that are grouped together, and make a hypothesis that those datapoints have similar biological properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd2V_Ms9Ejfo"
      },
      "source": [
        "## Visualize your data in principle component space\n",
        "\n",
        "We can use ```sklearn```'s ```decomposition.PCA``` class to compute the principle components of a dataset and visualize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0Ha3p29e_Mj",
        "cellView": "form"
      },
      "source": [
        "#@title ### **Exercise: Run the following to visualize our X dataset in principal component space.**\n",
        "data = mutation_df\n",
        "pca = decomposition.PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "df = pd.DataFrame()\n",
        "df['Principal Component 1'] = [pc[0] for pc in pca.transform(X)]\n",
        "df['Principal Component 2'] = [pc[1] for pc in pca.transform(X)]\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.scatterplot(data=df, x='Principal Component 1', y='Principal Component 2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gKgNY92GJmA",
        "cellView": "form"
      },
      "source": [
        "# @title ### **Exercise: Why do you think this is called unsupervised learning?**\n",
        "Response = \"\" #@param {type:\"string\"}\n",
        "print(\"We are able to make obsevations about the data without ever using any sort of label.\")\n",
        "print(\"We can see that there are clusters of very similar samples.  \")\n",
        "print(\"We should suspect data points near each other are probably viruses \")\n",
        "print(\"from similar regions of the world, or that these viruses have similar biological properties.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldo1k1FRHt1n"
      },
      "source": [
        "*Hint: Look at the hidden code. Did we ever use ```Y```?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8VciL0LF01n"
      },
      "source": [
        "Just for fun, we are going to now color the data points on our PCA plot to see if points that are similar in PC space are from similar regions of the world. Run the cell below to do so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drhEt1Bdgft4",
        "cellView": "form"
      },
      "source": [
        "#@title ### **Exercise: Color the data points by region**\n",
        "df['color'] = Y\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.scatterplot(data=df, x='Principal Component 1', y='Principal Component 2', hue='color')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca-QAE9cFRI7",
        "cellView": "form"
      },
      "source": [
        "# @title ### **Exercise: Based on your observations about the PCA plot, can you hypothesize why our classifier wasn't 100% accurate?**\n",
        "Hypothesis = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuJGGokXHxkF"
      },
      "source": [
        "# Wrapping things up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zTPn0z2H0Kr"
      },
      "source": [
        "**Amazing job!!**  What we went over today is normally taught in a sophomore level college course.\n",
        "\n",
        "<a href=\"https://imgflip.com/i/424k4k\"><img src=\"https://i.imgflip.com/424k4k.jpg\" title=\"made at imgflip.com\"/></a>\n",
        "\n",
        "To synthesize what we learned today, complete the following exercise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80I8WHgxItRg",
        "cellView": "form"
      },
      "source": [
        "#@title ### **Exercise: What other machine learning problems that we did in this course do you think would benefit from these advanced feature reduction techniques?**\n",
        "_1_ = \"\" #@param {type:\"string\"}\n",
        "_2_ = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}